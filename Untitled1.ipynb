{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04e60b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "821f8e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = Image.open(\"./dataset/gan-getting-started/photo/photo_jpg/000910d219.jpg\")\n",
    "style_image = Image.open(\"./dataset/gan-getting-started/monet/monet_jpg/000c1e3bff.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf79a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image):\n",
    "    image = transforms.ToTensor()(image)\n",
    "    image = image.unsqueeze(0)\n",
    "    image = image.to(device)\n",
    "    return image\n",
    "\n",
    "def normalize_image(image):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
    "    image = (image - mean[:, None, None]) / std[:, None, None]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c489961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaydenpark\\Anaconda3\\envs\\nlp3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jaydenpark\\Anaconda3\\envs\\nlp3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "vgg = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "\n",
    "def get_features(image, model):\n",
    "    layers = {\n",
    "        '0': 'conv1_1',\n",
    "        '5': 'conv2_1',\n",
    "        '10': 'conv3_1',\n",
    "        '19': 'conv4_1',\n",
    "        '28': 'conv5_1'\n",
    "    }\n",
    "    features = {}\n",
    "    x = image\n",
    "    for name, layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "        if name in layers:\n",
    "            features[layers[name]] = x\n",
    "    return features\n",
    "\n",
    "content = load_image(content_image)\n",
    "style = load_image(style_image)\n",
    "content_features = get_features(normalize_image(content), vgg)\n",
    "style_features = get_features(normalize_image(style), vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "758f54b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(tensor):\n",
    "    _, c, h, w = tensor.size()\n",
    "    tensor = tensor.view(c, h * w)\n",
    "    gram = torch.mm(tensor, tensor.t())\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7227fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = content.clone().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97de5b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_weight = 1\n",
    "style_weight = 10\n",
    "optimizer = optim.Adam([target], lr=0.002)\n",
    "steps = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32ab56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [1/10000], Total Loss: 2923044864.0000, Content Loss: 0.0000, Style Loss: 292304480.0000\n",
      "Step [101/10000], Total Loss: 188212960.0000, Content Loss: 1.8690, Style Loss: 18821296.0000\n",
      "Step [201/10000], Total Loss: 119511128.0000, Content Loss: 2.0679, Style Loss: 11951113.0000\n",
      "Step [301/10000], Total Loss: 94424560.0000, Content Loss: 2.1746, Style Loss: 9442456.0000\n",
      "Step [401/10000], Total Loss: 78702088.0000, Content Loss: 2.2341, Style Loss: 7870208.5000\n",
      "Step [501/10000], Total Loss: 65855664.0000, Content Loss: 2.2863, Style Loss: 6585566.0000\n",
      "Step [601/10000], Total Loss: 54105988.0000, Content Loss: 2.3231, Style Loss: 5410598.5000\n",
      "Step [701/10000], Total Loss: 43633892.0000, Content Loss: 2.3469, Style Loss: 4363389.0000\n",
      "Step [801/10000], Total Loss: 35300180.0000, Content Loss: 2.3680, Style Loss: 3530017.7500\n",
      "Step [901/10000], Total Loss: 29145512.0000, Content Loss: 2.3878, Style Loss: 2914551.0000\n",
      "Step [1001/10000], Total Loss: 24690344.0000, Content Loss: 2.4068, Style Loss: 2469034.2500\n",
      "Step [1101/10000], Total Loss: 21416182.0000, Content Loss: 2.4255, Style Loss: 2141618.0000\n",
      "Step [1201/10000], Total Loss: 18935458.0000, Content Loss: 2.4453, Style Loss: 1893545.5000\n",
      "Step [1301/10000], Total Loss: 16988122.0000, Content Loss: 2.4557, Style Loss: 1698812.0000\n",
      "Step [1401/10000], Total Loss: 15406342.0000, Content Loss: 2.4601, Style Loss: 1540634.0000\n",
      "Step [1501/10000], Total Loss: 14086991.0000, Content Loss: 2.4664, Style Loss: 1408698.8750\n",
      "Step [1601/10000], Total Loss: 12972877.0000, Content Loss: 2.4708, Style Loss: 1297287.5000\n",
      "Step [1701/10000], Total Loss: 12006831.0000, Content Loss: 2.4788, Style Loss: 1200682.8750\n",
      "Step [1801/10000], Total Loss: 11148983.0000, Content Loss: 2.4861, Style Loss: 1114898.1250\n",
      "Step [1901/10000], Total Loss: 10388368.0000, Content Loss: 2.4920, Style Loss: 1038836.5625\n",
      "Step [2001/10000], Total Loss: 9699399.0000, Content Loss: 2.4988, Style Loss: 969939.6875\n",
      "Step [2101/10000], Total Loss: 9068630.0000, Content Loss: 2.5049, Style Loss: 906862.6875\n",
      "Step [2201/10000], Total Loss: 8489812.0000, Content Loss: 2.5092, Style Loss: 848980.8750\n",
      "Step [2301/10000], Total Loss: 7952933.0000, Content Loss: 2.5143, Style Loss: 795293.0625\n",
      "Step [2401/10000], Total Loss: 7449495.0000, Content Loss: 2.5207, Style Loss: 744949.2500\n",
      "Step [2501/10000], Total Loss: 6978674.5000, Content Loss: 2.5258, Style Loss: 697867.1875\n",
      "Step [2601/10000], Total Loss: 6542268.0000, Content Loss: 2.5264, Style Loss: 654226.5625\n",
      "Step [2701/10000], Total Loss: 6139051.5000, Content Loss: 2.5286, Style Loss: 613904.8750\n",
      "Step [2801/10000], Total Loss: 5764546.5000, Content Loss: 2.5290, Style Loss: 576454.3750\n",
      "Step [2901/10000], Total Loss: 5417915.5000, Content Loss: 2.5309, Style Loss: 541791.3125\n",
      "Step [3001/10000], Total Loss: 5092347.0000, Content Loss: 2.5310, Style Loss: 509234.4688\n",
      "Step [3101/10000], Total Loss: 4787053.0000, Content Loss: 2.5345, Style Loss: 478705.0625\n",
      "Step [3201/10000], Total Loss: 4501247.0000, Content Loss: 2.5345, Style Loss: 450124.4375\n",
      "Step [3301/10000], Total Loss: 4232919.5000, Content Loss: 2.5363, Style Loss: 423291.6875\n",
      "Step [3401/10000], Total Loss: 3984132.0000, Content Loss: 2.5373, Style Loss: 398412.9375\n",
      "Step [3501/10000], Total Loss: 3752417.2500, Content Loss: 2.5391, Style Loss: 375241.4688\n",
      "Step [3601/10000], Total Loss: 3530400.5000, Content Loss: 2.5390, Style Loss: 353039.8125\n",
      "Step [3701/10000], Total Loss: 3319439.0000, Content Loss: 2.5428, Style Loss: 331943.6562\n",
      "Step [3801/10000], Total Loss: 3125904.0000, Content Loss: 2.5434, Style Loss: 312590.1562\n",
      "Step [3901/10000], Total Loss: 2947781.5000, Content Loss: 2.5458, Style Loss: 294777.9062\n",
      "Step [4001/10000], Total Loss: 2784818.0000, Content Loss: 2.5474, Style Loss: 278481.5625\n",
      "Step [4101/10000], Total Loss: 2632779.7500, Content Loss: 2.5479, Style Loss: 263277.7188\n",
      "Step [4201/10000], Total Loss: 2490046.0000, Content Loss: 2.5506, Style Loss: 249004.3594\n",
      "Step [4301/10000], Total Loss: 2362738.2500, Content Loss: 2.5527, Style Loss: 236273.5781\n",
      "Step [4401/10000], Total Loss: 2226759.5000, Content Loss: 2.5563, Style Loss: 222675.7031\n",
      "Step [4501/10000], Total Loss: 2114863.0000, Content Loss: 2.5605, Style Loss: 211486.0625\n",
      "Step [4601/10000], Total Loss: 1996097.8750, Content Loss: 2.5640, Style Loss: 199609.5312\n",
      "Step [4701/10000], Total Loss: 1898892.1250, Content Loss: 2.5649, Style Loss: 189888.9531\n",
      "Step [4801/10000], Total Loss: 1789347.1250, Content Loss: 2.5651, Style Loss: 178934.4531\n",
      "Step [4901/10000], Total Loss: 1700550.6250, Content Loss: 2.5657, Style Loss: 170054.7969\n",
      "Step [5001/10000], Total Loss: 1606295.6250, Content Loss: 2.5697, Style Loss: 160629.2969\n",
      "Step [5101/10000], Total Loss: 1524302.5000, Content Loss: 2.5714, Style Loss: 152429.9844\n",
      "Step [5201/10000], Total Loss: 1447679.6250, Content Loss: 2.5730, Style Loss: 144767.7031\n",
      "Step [5301/10000], Total Loss: 1379843.7500, Content Loss: 2.5761, Style Loss: 137984.1094\n",
      "Step [5401/10000], Total Loss: 1307429.6250, Content Loss: 2.5775, Style Loss: 130742.7031\n",
      "Step [5501/10000], Total Loss: 1248964.8750, Content Loss: 2.5792, Style Loss: 124896.2188\n",
      "Step [5601/10000], Total Loss: 1215452.1250, Content Loss: 2.5799, Style Loss: 121544.9531\n",
      "Step [5701/10000], Total Loss: 1145335.0000, Content Loss: 2.5813, Style Loss: 114533.2344\n",
      "Step [5801/10000], Total Loss: 1108347.8750, Content Loss: 2.5806, Style Loss: 110834.5234\n",
      "Step [5901/10000], Total Loss: 1201307.2500, Content Loss: 2.5863, Style Loss: 120130.4609\n",
      "Step [6001/10000], Total Loss: 1006006.8125, Content Loss: 2.5840, Style Loss: 100600.4219\n",
      "Step [6101/10000], Total Loss: 969958.1875, Content Loss: 2.5850, Style Loss: 96995.5625\n",
      "Step [6201/10000], Total Loss: 1029694.0625, Content Loss: 2.5838, Style Loss: 102969.1484\n",
      "Step [6301/10000], Total Loss: 924379.6875, Content Loss: 2.5862, Style Loss: 92437.7109\n",
      "Step [6401/10000], Total Loss: 919647.1875, Content Loss: 2.5874, Style Loss: 91964.4609\n",
      "Step [6501/10000], Total Loss: 881122.8125, Content Loss: 2.5867, Style Loss: 88112.0234\n",
      "Step [6601/10000], Total Loss: 831260.9375, Content Loss: 2.5862, Style Loss: 83125.8359\n",
      "Step [6701/10000], Total Loss: 802050.7500, Content Loss: 2.5863, Style Loss: 80204.8203\n",
      "Step [6801/10000], Total Loss: 808666.0625, Content Loss: 2.5879, Style Loss: 80866.3516\n",
      "Step [6901/10000], Total Loss: 762106.8125, Content Loss: 2.5896, Style Loss: 76210.4219\n",
      "Step [7001/10000], Total Loss: 772438.9375, Content Loss: 2.5882, Style Loss: 77243.6406\n",
      "Step [7101/10000], Total Loss: 718157.6250, Content Loss: 2.5889, Style Loss: 71815.5078\n",
      "Step [7201/10000], Total Loss: 708218.4375, Content Loss: 2.5898, Style Loss: 70821.5859\n",
      "Step [7301/10000], Total Loss: 691742.6875, Content Loss: 2.5898, Style Loss: 69174.0156\n"
     ]
    }
   ],
   "source": [
    "for i in range(steps):\n",
    "    target_features = get_features(normalize_image(target), vgg)\n",
    "    content_loss = torch.mean((target_features['conv5_1'] - content_features['conv5_1']) ** 2)\n",
    "    style_loss = 0\n",
    "    for layer in style_features:\n",
    "        target_feature = target_features[layer]\n",
    "        target_gram = gram_matrix(target_feature)\n",
    "        style_gram = gram_matrix(style_features[layer])\n",
    "        layer_loss = torch.mean((target_gram - style_gram) ** 2)\n",
    "        style_loss += layer_loss\n",
    "    total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    if i % 100 == 0:\n",
    "        print(\"Step [{}/{}], Total Loss: {:.4f}, Content Loss: {:.4f}, Style Loss: {:.4f}\"\n",
    "              .format(i + 1, steps, total_loss.item(), content_loss.item(), style_loss.item()))\n",
    "        output = target.detach().clone().cpu()\n",
    "        output = output.squeeze(0)\n",
    "        output = transforms.ToPILImage()(output)\n",
    "        output.save(\"output/output-{}.jpg\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c68234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
